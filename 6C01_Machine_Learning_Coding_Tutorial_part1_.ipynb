{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexisf125/alexisf125/blob/main/6C01_Machine_Learning_Coding_Tutorial_part1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outline for Part 1\n",
        "\n",
        "- Exploratory Data Analysis\n",
        "- Data Pre-Processing\n",
        "- Model Training and Evaluating\n",
        "- Pipelines\n",
        "- Cross Validation and Hyperparameter Search\n",
        "\n",
        "We will use the logistic regression model on Iris flower dataset as a running example.\n",
        "\n",
        "Libraries used:\n",
        "* [Pandas](https://pandas.pydata.org/) for data analysis and manipulation\n",
        "* [Seaborn](https://seaborn.pydata.org/) and [Matplotlib](https://matplotlib.org/) for data visualization\n",
        "* [Scikit-Learn](https://scikit-learn.org/stable/) for model construction, training, evaluation and hyperparameter selection\n",
        "* [NumPy](https://numpy.org/) for numerical computation. Makes computation with vectors and matrices (represented as NumPy arrays) fast and easy.\n"
      ],
      "metadata": {
        "id": "wgzuj40Yz6yT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US8Yq1Gn8UIj"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "In this section, we'll load and visualize the data.\n",
        "\n",
        "We will use a small toy dataset called the Iris plants dataset. It contains 150 examples of Iris flowers. The input variables are attributes of the Iris (e.g., sepal length, petal width). The prediction targets consist of three categories (classes) of Irises: Setosa, Versicolour, and Virginica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcXao5-G8Ozo"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(0) # set random seed to make results deterministic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KximeeUl-ceH"
      },
      "outputs": [],
      "source": [
        "# load the example dataset\n",
        "dataset = datasets.load_iris(as_frame=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mifNNj4OE2Bs"
      },
      "source": [
        "We will store the data as a table through the [Pandas DataFrame]((https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) data structure. Each example in the dataset corresponds to a row, and the attributes (X and y values) correspond to columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwdgcq9SDV4e"
      },
      "outputs": [],
      "source": [
        "dataset_df = dataset.frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh0fctMUDrMT"
      },
      "outputs": [],
      "source": [
        "# use pandas .head() to visualize the first 5 rows in the dataframe\n",
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJoI2fa4FoB6"
      },
      "outputs": [],
      "source": [
        "# we can use the .info() method to print a concise summary of the dataframe\n",
        "dataset_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show target classes\n",
        "dataset_df['target'].unique()"
      ],
      "metadata": {
        "id": "pDkU1oDd5Pbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use the .describe() method to print summary statistics of the dataframe\n",
        "dataset_df.describe().round(2)"
      ],
      "metadata": {
        "id": "wTQS3tmckuQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ljJ2S2QF5DV"
      },
      "outputs": [],
      "source": [
        "# we compute the pairwise correlation of columns in the matrix using .corr()\n",
        "# note: we do this only for the features, since the targets are categorical variables\n",
        "feature_cols = dataset_df.columns[:-1]  # exclude the last column\n",
        "correlation_matrix = dataset_df[feature_cols].corr().round(2)\n",
        "correlation_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKUKvoVwGeTD"
      },
      "source": [
        "We can use the Seaborn library to visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnjr1uLJGYs_"
      },
      "outputs": [],
      "source": [
        "# get color pallete to use to visualize positive correlations in red and negative correlations in blue\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(data=correlation_matrix, annot=True, cmap=cmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3WGJqxxHGfd"
      },
      "outputs": [],
      "source": [
        "# plot pairwise relationships in the data\n",
        "sns.pairplot(dataset_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TvPtDcBIEb5"
      },
      "source": [
        "# Data Pre-Processing\n",
        "\n",
        "In this section, we will split the data into train, validation, and test groups, and apply min-max normalization.\n",
        "\n",
        "Libraries used:\n",
        "* [Scikit-Learn](https://scikit-learn.org/stable/)\n",
        "\n",
        "Useful links:\n",
        "* [Scikit-Learn transformations](https://scikit-learn.org/stable/data_transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIX6gldNVSmd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uj79gjwIC54"
      },
      "outputs": [],
      "source": [
        "# separate out features and targets\n",
        "X, y = dataset_df[feature_cols].to_numpy(), dataset_df['target'].to_numpy()  # .to_numpy() converts from dataframe to numpy matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cUXHhRGVPpD"
      },
      "outputs": [],
      "source": [
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdMXi3zVn-Q"
      },
      "outputs": [],
      "source": [
        "# apply min-max normalization: scale data to be in 0-1 range\n",
        "\n",
        "# find normalization paramters (i.e., max and min value) using TRAIN data\n",
        "mms = MinMaxScaler()\n",
        "X_train = mms.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZuH8VusivjX"
      },
      "outputs": [],
      "source": [
        "# summary stats of scaled X_train\n",
        "pd.DataFrame(X_train).describe().round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAV3ATfFivjX"
      },
      "outputs": [],
      "source": [
        "# transform test data\n",
        "X_test = mms.transform(X_test)\n",
        "pd.DataFrame(X_test).describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP7s_TUkhZpH"
      },
      "source": [
        "# Train and Evaluate Logistic Regression Model with Scikit-Learn\n",
        "\n",
        "In this section, we will\n",
        "- Use Scikit-Learn to train and evaluate a Logistic Regression model.\n",
        "- Bundle processing steps with sklearn pipelines.\n",
        "\n",
        "Useful links:\n",
        "- [Scikit-Learn Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "- [Scikit-Learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) for evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK4ERf-zjbH4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEaUhBq_i96p"
      },
      "source": [
        "First, a quick example of how to train and evaluate a model without hyperparameter sweeping/selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBKvmX25jZcc"
      },
      "outputs": [],
      "source": [
        "# model construction\n",
        "model = LogisticRegression(\n",
        "    C=1.0, \n",
        "    fit_intercept=True, \n",
        "    solver='lbfgs', \n",
        "    multi_class='multinomial'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can fit the parameters by calling the .fit() method\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on new data\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "6pMS7c42onfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrCckCzeivjY"
      },
      "outputs": [],
      "source": [
        "# get fitted parameters\n",
        "model.coef_, model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asiA2LSdivjY"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# display true vs. predicted classes\n",
        "print(f'true:\\t\\t{y_test}\\nprediction:\\t{y_pred}')"
      ],
      "metadata": {
        "id": "WDD86pDn_HXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use accuracy_score() to score predictions against true data\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f'accuracy:\\t{accuracy_score(y_test, y_pred)}')"
      ],
      "metadata": {
        "id": "7QSla59SojIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For later use, we can also make a **scorer object** which scores a model against a dataset."
      ],
      "metadata": {
        "id": "dG0tBZPFAg8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjxbU7uyivjY"
      },
      "outputs": [],
      "source": [
        "# use a scorer to calculate test set accuracy\n",
        "from sklearn.metrics import make_scorer\n",
        "scorer = make_scorer(accuracy_score)\n",
        "scorer(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qkGTkR4oivjY"
      },
      "outputs": [],
      "source": [
        "# visualize frequencies of true vs. predicted classes\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(cmap='Blues')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "X42lPGQ_ivjZ"
      },
      "source": [
        "# Pipelines\n",
        "\n",
        "In this section, we will use [sklearn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to bundle processing steps into a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_WJGVBFivjY"
      },
      "source": [
        "### Pipeline and cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgvMckM3ivjY"
      },
      "outputs": [],
      "source": [
        "# create a pipeline model to bundle together processing steps\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([(\"MinMaxScaler\", MinMaxScaler()), (\"model\", model)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCPmhrgRivjY"
      },
      "outputs": [],
      "source": [
        "# fit the pipeline with unscaled training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXEdaWhlivjZ"
      },
      "outputs": [],
      "source": [
        "# compute prediction score for fitted pipeline\n",
        "scorer(pipeline, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Search via Cross Validation\n",
        "In this section, we will:\n",
        "- Perform k-fold cross validation on the pipelined model.\n",
        "- Use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) perform a grid search over possible hyperparameter settings."
      ],
      "metadata": {
        "id": "7e6AIBiiF8-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAeP-aU3ivjZ"
      },
      "outputs": [],
      "source": [
        "# cross validation score for a single hyperparameter setting\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(pipeline, X_train, y_train, cv=5, scoring=scorer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeD0YJBPivjZ"
      },
      "source": [
        "### Grid Search CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ5cSjYTivjZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = [\n",
        "    # hyperparameter grid 1\n",
        "    {\n",
        "        'model__fit_intercept': [True, False],\n",
        "        'model__C': [.5, 1, 2],\n",
        "        'model__penalty': ['l2'],\n",
        "        'model__solver': ['lbfgs']\n",
        "    },\n",
        "    # hyperparameter grid 2\n",
        "    {\n",
        "        'model__fit_intercept': [True, False],\n",
        "        'model__C': [.5, 1, 2],\n",
        "        'model__penalty': ['l1'],\n",
        "        'model__solver': ['saga'],\n",
        "        'model__max_iter': [5000]\n",
        "    }]\n",
        "\n",
        "search = GridSearchCV(estimator=pipeline, param_grid=param_grid, n_jobs=-1, cv=5, scoring=scorer)\n",
        "\n",
        "# Use `pipeline.get_params()` to show available pipeline hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCoiebdRivjZ"
      },
      "outputs": [],
      "source": [
        "# sweep over all hyperparameter grids, fit model for each setting\n",
        "search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ILA0SF8ivjZ"
      },
      "outputs": [],
      "source": [
        "# display best model hyperparameters\n",
        "print(f\"Best parameters: {search.best_params_}:\")\n",
        "print(f\"Best parameter CV Score: {search.best_score_}:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uQKUJ0bivja"
      },
      "outputs": [],
      "source": [
        "# get all hyperparameter setting results\n",
        "results = pd.DataFrame(search.cv_results_)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nweWU_Afivja"
      },
      "outputs": [],
      "source": [
        "# Evaluate the best model on test data\n",
        "selected_model = search.best_estimator_\n",
        "print(f'accuracy: {scorer(selected_model, X_test, y_test)}\\n')\n",
        "print(f'confusion matrix:\\n{confusion_matrix(y_test, selected_model.predict(X_test))}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuabGy_IKwfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}